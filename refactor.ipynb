{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL EARTHQUAKE PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## packages and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "import pickle\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy import fftpack\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # may remove later\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set project parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a82f299afbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# edit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/notebooks/storage/earthquake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# edit\n",
    "PROJECT_DIR = Path('/notebooks/storage/earthquake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change `pwd` to `PROJECT_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/earthquake\n",
      "['test_samples.pickle', 'train.csv', 'consecutive_samples.pickle', 'submission.csv', 'model_xgb.pickle', 'refactor.ipynb', 'test_samples_eng.pickle', 'train.pickle', '.ipynb_checkpoints', 'test']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download/unzip/load data from kaggle\n",
    "- run once -> comment out\n",
    "- ADD: NOTE ON WHAT HAPPENS IF IT IS RUN TWICE, REPLACEMENT OR COLLISION ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !kaggle competitions download LANL-Earthquake-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unzip kaggle files\n",
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_zip='train.csv.zip'\n",
    "# test_zip='test.zip'\n",
    "# earthquake_dir='.'\n",
    "# test_dir='./test'\n",
    "\n",
    "# zip_ref=zipfile.ZipFile(train_zip,'r')\n",
    "# zip_ref.extractall(earthquake_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# zip_ref=zipfile.ZipFile(test_zip,'r')\n",
    "# zip_ref.extractall(test_dir)\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read train csv -> convert to dataframe -> pickle\n",
    "NOTE: run once -> pickle `train` -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "train = pd.read_csv('./train.csv', dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `train` dataframe\n",
    "- NOTE: run once then comment out\n",
    "- CAUTION !!! code below overwrites `train`!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('/notebooks/storage/earthquake/train.pickle','wb') as f:\n",
    "#     pickle.dump(train,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `train` dataframe from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load `train` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(PROJECT_DIR/'train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: manipulate train data to exploit leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate consecutive samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set `consecutive_sample` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "CONS_FILENAME = 'consecutive_samples.pickle'\n",
    "\n",
    "#leave as-is\n",
    "cons_filename = PROJECT_DIR/CONS_NAME+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConsecutive(train):\n",
    "    '''\n",
    "    input: train dataframe (dim: #entries x 2 features: acoustic_data/time_to_failure)\n",
    "    returns: dataframe with length/sample_length consecutive samples, each with:\n",
    "        sequence = np.array(150k,) and time_to_failure = float\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(length//sample_length):\n",
    "        \n",
    "        start = i*sample_length\n",
    "        end = (i+1)*sample_length\n",
    "        \n",
    "        samples += [[train['acoustic_data'].values[start:end],train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'time_to_failure'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "consecutive_samples = generateConsecutive(train)\n",
    "\n",
    "consecutive_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `consecutive_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(cons_filename,'wb') as f:\n",
    "    pickle.dump(consecutive_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `consecutive_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del consecutive_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `consecutive_samples` from PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(cons_filename,'rb') as f:\n",
    "    consecutive_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate N random samples\n",
    "with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "RAND_FILENAME = 'random_samples.pickle'\n",
    "\n",
    "#leave unchanged\n",
    "rand_filename = PROJECT_DIR/CONS_NAME+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandom(train,n):\n",
    "    '''\n",
    "    input: train dataframe (dim: #entries x 2 features: acoustic_data/time_to_failure)\n",
    "    returns: dataframe with n random samples, each with:\n",
    "        sequence = np.array(150k,) and time_to_failure = float\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        start = random.randint(0,max_start)\n",
    "        end = start + sample_length\n",
    "        \n",
    "        samples += [[np.array(train['acoustic_data'].values[start:end]),train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'time_to_failure'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random_samples = generateRandom(train,10)\n",
    "\n",
    "random_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `random_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(rand_filename,'wb') as f:\n",
    "    pickle.dump(random_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `random_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del random_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `consecutive_samples` from `PROJECT_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(rand_filename,'rb') as f:\n",
    "    random_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full sequence features\n",
    "add features to a given sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullFeatures(samples_df):\n",
    "    \n",
    "    #defining x as matrix of sequence data\n",
    "    x = []\n",
    "    for index,row in samples_df.iterrows():\n",
    "        x += [samples_df.loc[index,'sequence']]    \n",
    "    x = np.stack(x)\n",
    "    x_abs = np.absolute(x)\n",
    "    \n",
    "    length=150000\n",
    "    \n",
    "    #non-abs\n",
    "    samples_df['mean'] = np.mean(x,axis=1)\n",
    "    samples_df['median'] = np.median(x,axis=1)\n",
    "    samples_df['std'] = np.std(x,axis=1)\n",
    "    \n",
    "    samples_df['kurtosis'] = stats.kurtosis(x,axis=1)   \n",
    "    samples_df['m2'] = stats.moment(x,axis=1,moment=2)\n",
    "    samples_df['m3'] = stats.moment(x,axis=1,moment=3)\n",
    "    samples_df['skew'] = stats.skew(x,axis=1)\n",
    "    samples_df['variation'] = stats.variation(x,axis=1)\n",
    "    samples_df['sem'] = stats.sem(x,axis=1)\n",
    "\n",
    "    samples_df['iqr25_75'] = stats.iqr(x,axis=1,rng=(25,75))\n",
    "    samples_df['iqr10_90'] = stats.iqr(x,axis=1,rng=(10,90))\n",
    "    samples_df['iqr5_95'] = stats.iqr(x,axis=1,rng=(5,95))\n",
    "    samples_df['iqr1_99'] = stats.iqr(x,axis=1,rng=(1,99))\n",
    "    \n",
    "    \n",
    "    #abs\n",
    "    samples_df['mean_abs'] = np.mean(x_abs,axis=1)\n",
    "    samples_df['median_abs'] = np.median(x_abs,axis=1)\n",
    "    samples_df['std_abs'] = np.std(x_abs,axis=1)\n",
    "    \n",
    "    samples_df['kurtosis_abs'] = stats.kurtosis(x_abs,axis=1)   \n",
    "    samples_df['m2_abs'] = stats.moment(x_abs,axis=1,moment=2)\n",
    "    samples_df['m3_abs'] = stats.moment(x_abs,axis=1,moment=3)\n",
    "    samples_df['skew_abs'] = stats.skew(x_abs,axis=1)\n",
    "    samples_df['variation_abs'] = stats.variation(x_abs,axis=1)\n",
    "    samples_df['sem_abs'] = stats.sem(x_abs,axis=1)\n",
    "\n",
    "    \n",
    "    samples_df['iqr25_75_abs'] = stats.iqr(x_abs,axis=1,rng=(25,75))\n",
    "    samples_df['iqr10_90_abs'] = stats.iqr(x_abs,axis=1,rng=(10,90))\n",
    "    samples_df['iqr5_95_abs'] = stats.iqr(x_abs,axis=1,rng=(5,95))\n",
    "    samples_df['iqr1_99_abs'] = stats.iqr(x_abs,axis=1,rng=(1,99))\n",
    "    \n",
    "    \n",
    "    #slices\n",
    "    slices_list = [2,4]\n",
    "    \n",
    "    for slices in slices_list:\n",
    "\n",
    "        for i in range(slices):\n",
    "\n",
    "            suffix = '_'+str(slices)+'_'+str(i+1)\n",
    "\n",
    "            #create same as above, but for first half and second half\n",
    "            \n",
    "            x_slice = x[:,i*(length//slices):(i+1)*(length//slices)]\n",
    "            x_abs_slice = np.absolute(x_slice)\n",
    "\n",
    "            #non-abs\n",
    "            samples_df['mean'+suffix] = np.mean(x_slice,axis=1)\n",
    "            samples_df['median'+suffix] = np.median(x_slice,axis=1)\n",
    "            samples_df['std'+suffix] = np.std(x_slice,axis=1)\n",
    "\n",
    "            samples_df['kurtosis'+suffix] = stats.kurtosis(x_slice,axis=1)\n",
    "            samples_df['m2'+suffix] = stats.moment(x_slice,axis=1,moment=2)\n",
    "            samples_df['m3'+suffix] = stats.moment(x_slice,axis=1,moment=3)\n",
    "            samples_df['skew'+suffix] = stats.skew(x_slice,axis=1)\n",
    "            samples_df['variation'+suffix] = stats.variation(x_slice,axis=1)\n",
    "            samples_df['sem'+suffix] = stats.sem(x_slice,axis=1)\n",
    "\n",
    "            samples_df['iqr25_75'+suffix] = stats.iqr(x_slice,axis=1,rng=(25,75))\n",
    "            samples_df['iqr10_90'+suffix] = stats.iqr(x_slice,axis=1,rng=(10,90))\n",
    "            samples_df['iqr5_95'+suffix] = stats.iqr(x_slice,axis=1,rng=(5,95))\n",
    "            samples_df['iqr1_99'+suffix] = stats.iqr(x_slice,axis=1,rng=(1,99))\n",
    "\n",
    "\n",
    "            #abs\n",
    "            samples_df['mean_abs'+suffix] = np.mean(x_abs_slice,axis=1)\n",
    "            samples_df['median_abs'+suffix] = np.median(x_abs_slice,axis=1)\n",
    "            samples_df['std_abs'+suffix] = np.std(x_abs_slice,axis=1)\n",
    "\n",
    "            samples_df['kurtosis_abs'+suffix] = stats.kurtosis(x_abs_slice,axis=1)\n",
    "            samples_df['m2_abs'+suffix] = stats.moment(x_abs_slice,axis=1,moment=2)\n",
    "            samples_df['m3_abs'+suffix] = stats.moment(x_abs_slice,axis=1,moment=3)\n",
    "            samples_df['skew_abs'+suffix] = stats.skew(x_abs_slice,axis=1)\n",
    "            samples_df['variation_abs'+suffix] = stats.variation(x_abs_slice,axis=1)\n",
    "            samples_df['sem_abs'+suffix] = stats.sem(x_abs_slice,axis=1)\n",
    "\n",
    "\n",
    "            samples_df['iqr25_75_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(25,75))\n",
    "            samples_df['iqr10_90_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(10,90))\n",
    "            samples_df['iqr5_95_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(5,95))\n",
    "            samples_df['iqr1_99_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(1,99))            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate features for a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3fd00ed07471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurrent_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcurrent_samples_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'random_samples'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_samples' is not defined"
     ]
    }
   ],
   "source": [
    "#edit\n",
    "eng_samples = random_samples\n",
    "eng_samples_name = 'random_samples''\n",
    "\n",
    "#leave as-is\n",
    "eng_samples_filename = PROJECT_DIR/eng_samples_name+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generateFullFeatures(eng_samples)\n",
    "\n",
    "eng_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `eng_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(eng_samples_filename,'wb') as f:\n",
    "    pickle.dump(eng_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `eng_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del eng_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `eng_samples` from PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(eng_samples_filename,'rb') as f:\n",
    "    eng_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.plot(x='iqr1_99_abs',y='time_to_failure',kind='scatter',logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.plot.hexbin(x='mean',y='time_to_failure',gridsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(consecutive_samples['time_to_failure'],consecutive_samples['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(consecutive_samples.time_to_failure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='iqr1_99_abs',y='time_to_failure',data=consecutive_samples,kind='hex',gridsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(consecutive_samples[['time_to_failure','iqr1_99_abs','iqr5_95_abs','iqr10_90_abs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into ggplot for grammar of graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_mean = 4.017 \n",
    "# train_mean = 5.68\n",
    "# mul_factor = public_mean/train_mean\n",
    "\n",
    "X = random_samples.drop(labels=['time_to_failure','sequence'],axis=1)\n",
    "Y = random_samples['time_to_failure']/mul_factor\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y,test_size=.1,random_state=0,shuffle=False)\n",
    "\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10000\n",
    "learning_rate = .001\n",
    "n_jobs = 8\n",
    "\n",
    "early_stopping_rounds = 20\n",
    "eval_set = [(x_val,y_val)]\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators = n_estimators, \n",
    "    learning_rate = learning_rate, \n",
    "    n_job = n_jobs\n",
    ")\n",
    "\n",
    "model.fit(x_train,y_train,\n",
    "              early_stopping_rounds= early_stopping_rounds,\n",
    "              eval_set = eval_set,\n",
    "              eval_metric = 'mae',\n",
    "              verbose = verbose\n",
    "             )\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "predictions = model.predict(x_val)\n",
    "print('mae : '+str(mean_absolute_error(predictions,y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "MODEL_NAME = 'model1.pickle'\n",
    "\n",
    "#leave as-is\n",
    "model_filename = PROJECT_DIR/MODEL_NAME+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(model_filename,'wb') as f:\n",
    "    pickle.dump(model,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `model` from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(model_filename,'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create submission\n",
    "#### start by creating a dataframe identical in format to consecutive/random samples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "TEST_SAMPLES_NAME = 'test_samples'\n",
    "\n",
    "# leave as-is\n",
    "test_samples_filename = PROJECT_DIR/'test'/TEST_SAMPLES_NAME+'.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.chdir(PROJECT_DIR/'test')\n",
    "    \n",
    "test_samples = pd.DataFrame(columns=['sequence','time_to_failure'])\n",
    "\n",
    "for i in range(len(os.listdir())):\n",
    "    \n",
    "    test_file = os.listdir()[i]\n",
    "    \n",
    "    temp_df = pd.read_csv('./'+test_file,engine='python')\n",
    "        \n",
    "    test_samples.loc[i,'sequence'] = np.array(temp_df['acoustic_data'].values[:])\n",
    "\n",
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `test_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(test_samples_filename,'wb') as f:\n",
    "    pickle.dump(test_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `test_samples` from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load `test_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(test_samples_filename,'rb') as f:\n",
    "    test_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "generateFullFeatures(test_samples)\n",
    "\n",
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `test_samples_eng`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open(PROJECT_DIR/'test'/TEST_SAMPLES_NAME+'_eng','wb') as f:\n",
    "#     pickle.dump(test_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `test_samples_eng` from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load `test_samples_eng`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(PROJECT_DIR/'test'/TEST_SAMPLES_NAME+'_eng','rb') as f:\n",
    "    test_samples = pickle.load(f)\n",
    "\n",
    "test_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions/submit\n",
    "This code will create a `submission.csv` file, that can be submitted to kaggle using the command line api, or website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_samples.drop(columns=['sequence','time_to_failure'])\n",
    "\n",
    "y_pred = model_xgb.predict(test_x)\n",
    "\n",
    "y_pred.shape\n",
    "\n",
    "submission_df = pd.DataFrame(columns=['seg_id','time_to_failure'])\n",
    "\n",
    "submission_df['seg_id'] = pd.Series([i[:-4] for i in os.listdir(PROJECT_DIR/'test')])\n",
    "submission_df.shape\n",
    "\n",
    "submission_df['time_to_failure'] = y_pred\n",
    "\n",
    "submission_df.tail()\n",
    "\n",
    "submission_df.to_csv(PROJECT_DIR/'submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
