{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "- remove hardcoding of folders/filenames\n",
    "- change filessystem to pathlib\n",
    "- remove unnecessary packages\n",
    "- make markdown more comprehensive; include justification for design decision, such as when/when not to copy/paste\n",
    "- improve comments: remove, cut down, add extra for unintuitive functions, like matrix loop at top of featuregen\n",
    "- refactor slice code in generateFullFeatures()\n",
    "- ???? design decision: to consolidated to into a singletest_samples_eng\n",
    "- is there a more elegant way to delete object from ram than object = None\n",
    "- include in readme: system limitations, dependencies, brief overview\n",
    "- dependency graph on github\n",
    "- reorder this notes \n",
    "- experiment fft, etc\n",
    "- split pickling concsecutive/random sample pre-and post- feature generation\n",
    "- edit the data visualisation to focus in on useful insights; move it to after running feature_importance on xgboost\n",
    "- work on adjustment/justification for test_set data-leakage\n",
    "- eliminate double -writing of xgboost params\n",
    "- more explicit warnings for overwriting pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "- remove hardcoding of folders/filenames\n",
    "- change filessystem to pathlib\n",
    "- remove unnecessary packages\n",
    "- make markdown more comprehensive; include justification for design decision, such as when/when not to copy/paste\n",
    "- improve comments: remove, cut down, add extra for unintuitive functions, like matrix loop at top of featuregen\n",
    "- refactor slice code in generateFullFeatures()\n",
    "- ???? design decision: to consolidated to into a singletest_samples_eng\n",
    "- is there a more elegant way to delete object from ram than object = None\n",
    "- include in readme: system limitations, dependencies, brief overview\n",
    "- dependency graph on github\n",
    "- reorder this notes \n",
    "- experiment fft, etc\n",
    "- split pickling concsecutive/random sample pre-and post- feature generation\n",
    "- edit the data visualisation to focus in on useful insights; move it to after running feature_importance on xgboost\n",
    "- work on adjustment/justification for test_set data-leakage\n",
    "- eliminate double -writing of xgboost params\n",
    "- more explicit warnings for overwriting pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#my imports\n",
    "import pathlib as path\n",
    "import zipfile\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from scipy import stats\n",
    "import scipy\n",
    "import statistics\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm code is run from folder /earthquake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/notebooks/storage/earthquake')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!kaggle competitions download LANL-Earthquake-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unzip kaggle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_zip='train.csv.zip'\n",
    "test_zip='test.zip'\n",
    "earthquake_dir='.'\n",
    "test_dir='./test'\n",
    "\n",
    "zip_ref=zipfile.ZipFile(train_zip,'r')\n",
    "zip_ref.extractall(earthquake_dir)\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref=zipfile.ZipFile(test_zip,'r')\n",
    "zip_ref.extractall(test_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert csv to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 13.6 s, total: 3min 14s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train = pd.read_csv('./train.csv', dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store train dataframe with pickle\n",
    "###### NOTE: run once then comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('/notebooks/storage/earthquake/train.pickle','wb') as f:\n",
    "#     pickle.dump(train,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NOTE: code below deletes loaded train object from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/train.pickle','rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering earthquakes to adjust mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_mean = 4.017 \n",
    "train_mean = 5.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earthquakes(train):\n",
    "    ttf = train['time_to_failure'].values\n",
    "    \n",
    "    curr_eqtime = ttf[0]\n",
    "    \n",
    "    eqs = list()\n",
    "    eqs = eqs + [(0,curr_eqtime)]\n",
    "    \n",
    "    for i in range(len(ttf)-1):\n",
    "        if ttf[i+1] - ttf[i] > 0:\n",
    "            eqs = eqs + [(i+1,ttf[i+1])]\n",
    "            \n",
    "    return eqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_to_failure'].tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.4691), (5656574, 11.5408), (50085878, 14.1806), (104677356, 8.8567), (138772453, 12.694), (187641820, 8.0555), (218652630, 7.059), (245829585, 16.1074), (307838917, 7.9056), (338276287, 9.6371), (375377848, 11.4264), (419368880, 11.0242), (461811623, 8.8281), (495800225, 8.566), (528777115, 14.7518), (585568144, 9.4595), (621985673, 11.6186)]\n"
     ]
    }
   ],
   "source": [
    "eq_list = earthquakes(train)\n",
    "print(eq_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629145480\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/notebooks/storage/earthquake/eq_list.pickle','wb') as f:\n",
    "#     pickle.dump(eq_list,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dd4bc7ee729d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/notebooks/storage/earthquake/eq_list.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0meq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open('/notebooks/storage/earthquake/eq_list.pickle','rb') as f:\n",
    "    eq_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_list_nums = list()\n",
    "\n",
    "for pair in eq_list:\n",
    "    eq_list_nums += [pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_list_nums_arr = np.array(eq_list_nums)\n",
    "print(eq_list_nums_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onethru17 = np.arange(17)\n",
    "np.mean(eq_list_nums_arr[1:16])/2\n",
    "\n",
    "print(5.336)\n",
    "\n",
    "eqs_chosen1 = [1,2,4,7,8,9,10,11,12,13,14,15] #5.67 #3,5,6\n",
    "eqs_chosen2 = [1,2,4,5,6,7,9,10,11,14,15] #5.72 #3,8,12,13\n",
    "eqs_chosen3 = [1,2,3,4,5,7,9,10,11,12,14,15] #5.69 #6,8,13\n",
    "eqs_chosen4 = [1,2,3,4,5,7,9,10,11,13,14,15] #5.68 #6,8,12\n",
    "# eqs_chosen5 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "# eqs_chosen6 = [1,2,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "\n",
    "count = 0\n",
    "for i in eqs_chosen5:\n",
    "    count += eq_list_nums_arr[i]\n",
    "    \n",
    "print(.5*count/len(eqs_chosen5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq0 = (0,5656574)\n",
    "eq3 = (104677355,138772454)\n",
    "eq5 = (187641819,218652631)\n",
    "eq6 = (218652629,245829586)\n",
    "eq8 = (307838916,338276288)\n",
    "eq12 = (461811622,495800226)\n",
    "eq13 = (495800224,528777116)\n",
    "eq16 = (621985672,629145480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nogo = list()\n",
    "nogo += [eq0]\n",
    "nogo += [eq16]\n",
    "nogo1 = nogo + [eq3] + [eq5] + [eq6]\n",
    "nogo2 = nogo + [eq3] + [eq8] + [eq12] + [eq13]\n",
    "nogo3 = nogo + [eq6] + [eq8] + [eq13]\n",
    "nogo4 = nogo + [eq6] + [eq8] + [eq12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate consecutive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConsecutive(train):\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(length//sample_length):\n",
    "        \n",
    "        start = i*sample_length\n",
    "        end = (i+1)*sample_length\n",
    "        \n",
    "        samples += [[train['acoustic_data'].values[start:end],train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'ttf'})\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "consecutive_samples = generateConsecutive(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate N random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandom(train,n):\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        start = random.randint(0,max_start)\n",
    "        end = start + sample_length\n",
    "        \n",
    "        samples += [[np.array(train['acoustic_data'].values[start:end]),train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'ttf'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_invalid_start(start,end,list_nogo):\n",
    "    cant_start = False\n",
    "    for interval in list_nogo:\n",
    "        if interval[0] < start and start < interval[1]:\n",
    "            cant_start = True\n",
    "        if inverval[0] < end and end < interval[1]:\n",
    "            cant_start = True\n",
    "    \n",
    "    return cant_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandom(train,n,list_nogo):\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        start = random.randint(0,max_start)\n",
    "        end = start + sample_length\n",
    "        \n",
    "        while is_invalid_start(start,end,list_nogo):\n",
    "            start = random.randint(0,max_start)\n",
    "            end = start + sample_length\n",
    "        \n",
    "        samples += [[np.array(train['acoustic_data'].values[start:end]),train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'ttf'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random_samples = generateRandom(train,3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,3,1,4,2,1,5,7,9,2,2,3])\n",
    "\n",
    "two_largest = heapq.nlargest(2,a)\n",
    "two_largest_idx = heapq.nlargest(2,range(len(a)),a.__getitem__)\n",
    "print(two_largest)\n",
    "print(two_largest_idx)\n",
    "\n",
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "b = np.array([[2,3,4],[5,6,7]])\n",
    "np.argmax(a,axis = 1)\n",
    "np.append(a,b,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full sequence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "import heapq\n",
    "\n",
    "def generateFullFeatures(samples_df):\n",
    "        \n",
    "    #defining x as matrix of sequence data\n",
    "    x = []\n",
    "    for index,row in samples_df.iterrows():\n",
    "        x += [samples_df.loc[index,'sequence']]    \n",
    "    x = np.stack(x)\n",
    "    x_abs = np.absolute(x)\n",
    "    x_realfft = np.real(fft(x))\n",
    "    \n",
    "    n = 20\n",
    "    \n",
    "    length=150000\n",
    "    \n",
    "    #non-abs\n",
    "#     samples_df['mean'] = np.mean(x,axis=1)\n",
    "#     samples_df['median'] = np.median(x,axis=1)\n",
    "    samples_df['std'] = np.std(x,axis=1)\n",
    "    \n",
    "#     samples_df['kurtosis'] = stats.kurtosis(x,axis=1)   \n",
    "#     samples_df['m2'] = stats.moment(x,axis=1,moment=2)\n",
    "#     samples_df['m3'] = stats.moment(x,axis=1,moment=3)\n",
    "#     samples_df['skew'] = stats.skew(x,axis=1)\n",
    "#     samples_df['variation'] = stats.variation(x,axis=1)\n",
    "#     samples_df['sem'] = stats.sem(x,axis=1)\n",
    "\n",
    "#     samples_df['iqr25_75'] = stats.iqr(x,axis=1,rng=(25,75))\n",
    "    samples_df['iqr10_90'] = stats.iqr(x,axis=1,rng=(10,90))\n",
    "#     samples_df['iqr5_95'] = stats.iqr(x,axis=1,rng=(5,95))\n",
    "#     samples_df['iqr1_99'] = stats.iqr(x,axis=1,rng=(1,99))\n",
    "    \n",
    "#     nlargest = np.apply_along_axis(lambda x: heapq.nlargest(n,x),1,x_realfft)\n",
    "#     nlargest_idx = np.apply_along_axis(lambda x: heapq.nlargest(n,range(len(x)),x.__getitem__),1,x_realfft)\n",
    "#     for i in range(n):\n",
    "#         samples_df['fft_altitude_' + str(i)] = nlargest[:,i]\n",
    "#         samples_df['fft_feq_' + str(i)] = nlargest_idx[:,i]\n",
    "\n",
    "    band1 = x_realfft[:,600:900]\n",
    "    band2 = x_realfft[:,1100:1450]\n",
    "    band3 = x_realfft[:,1450:1900]\n",
    "    band4 = x_realfft[:,1950:2800]\n",
    "    band5 = x_realfft[:,33000:42000]\n",
    "    \n",
    "    samples_df['band1_alt'] = np.max(band1,axis = 1)\n",
    "    samples_df['band1_freq'] = np.argmax(band1,axis = 1)\n",
    "    samples_df['band2_alt'] = np.max(band2,axis = 1)\n",
    "    samples_df['band2_freq'] = np.argmax(band2,axis = 1)\n",
    "    samples_df['band3_alt'] = np.max(band3,axis = 1)\n",
    "    samples_df['band3_freq'] = np.argmax(band3,axis = 1)\n",
    "    samples_df['band4_alt'] = np.max(band4,axis = 1)\n",
    "    samples_df['band4_freq'] = np.argmax(band4,axis = 1)\n",
    "    samples_df['band5_alt'] = np.max(band5,axis = 1)\n",
    "    samples_df['band5_freq'] = np.argmax(band5,axis = 1)\n",
    "    \n",
    "    #np.apply_along_axis\n",
    "    \n",
    "    #abs\n",
    "#     samples_df['mean_abs'] = np.mean(x_abs,axis=1)\n",
    "    samples_df['median_abs'] = np.median(x_abs,axis=1)\n",
    "    samples_df['std_abs'] = np.std(x_abs,axis=1)\n",
    "    \n",
    "#     samples_df['kurtosis_abs'] = stats.kurtosis(x_abs,axis=1)   \n",
    "#     samples_df['m2_abs'] = stats.moment(x_abs,axis=1,moment=2)\n",
    "#     samples_df['m3_abs'] = stats.moment(x_abs,axis=1,moment=3)\n",
    "#     samples_df['skew_abs'] = stats.skew(x_abs,axis=1)\n",
    "#     samples_df['variation_abs'] = stats.variation(x_abs,axis=1)\n",
    "#     samples_df['sem_abs'] = stats.sem(x_abs,axis=1)\n",
    "\n",
    "    \n",
    "#     samples_df['iqr25_75_abs'] = stats.iqr(x_abs,axis=1,rng=(25,75))\n",
    "    samples_df['iqr10_90_abs'] = stats.iqr(x_abs,axis=1,rng=(10,90))\n",
    "#     samples_df['iqr5_95_abs'] = stats.iqr(x_abs,axis=1,rng=(5,95))\n",
    "#     samples_df['iqr1_99_abs'] = stats.iqr(x_abs,axis=1,rng=(1,99))\n",
    "    \n",
    "    \n",
    "    #slices\n",
    "#     slices_list = [2,4]\n",
    "    slices_list = [2]\n",
    "    \n",
    "    for slices in slices_list:\n",
    "\n",
    "        for i in range(slices):\n",
    "\n",
    "            suffix = '_'+str(slices)+'_'+str(i+1)\n",
    "\n",
    "            #create same as above, but for first half and second half\n",
    "            \n",
    "            # STYLE: DEFINE sliced_x at top of inner loop to make code prettier\n",
    "\n",
    "            current_slice = x[:,i*(length//slices):(i+1)*(length//slices)]\n",
    "            current_slice_abs = x_abs[:,i*(length//slices):(i+1)*(length//slices)]\n",
    "            current_slice_realfft = np.real(fft(current_slice))\n",
    "            \n",
    "            #non-abs\n",
    "#             samples_df['mean'+suffix] = np.mean(current_slice,axis=1)\n",
    "#             samples_df['median'+suffix] = np.median(current_slice,axis=1)\n",
    "            samples_df['std'+suffix] = np.std(current_slice,axis=1)\n",
    "\n",
    "#             samples_df['kurtosis'+suffix] = stats.kurtosis(current_slice,axis=1)\n",
    "#             samples_df['m2'+suffix] = stats.moment(current_slice,axis=1,moment=2)\n",
    "#             samples_df['m3'+suffix] = stats.moment(current_slice,axis=1,moment=3)\n",
    "#             samples_df['skew'+suffix] = stats.skew(current_slice,axis=1)\n",
    "#             samples_df['variation'+suffix] = stats.variation(current_slice,axis=1)\n",
    "#             samples_df['sem'+suffix] = stats.sem(current_slice,axis=1)\n",
    "\n",
    "#             samples_df['iqr25_75'+suffix] = stats.iqr(current_slice,axis=1,rng=(25,75))\n",
    "            samples_df['iqr10_90'+suffix] = stats.iqr(current_slice,axis=1,rng=(10,90))\n",
    "#             samples_df['iqr5_95'+suffix] = stats.iqr(current_slice,axis=1,rng=(5,95))\n",
    "#             samples_df['iqr1_99'+suffix] = stats.iqr(current_slice,axis=1,rng=(1,99))\n",
    "            \n",
    "#             nlargest_slice = np.apply_along_axis(lambda x: heapq.nlargest(n,x),1,current_slice_realfft)\n",
    "#             nlargest_idx_slice = np.apply_along_axis(lambda x: heapq.nlargest(n,range(len(x)),x.__getitem__),1,current_slice_realfft)\n",
    "#             for i in range(n):\n",
    "#                 samples_df['fft_altitude_' + str(i)] = nlargest_slice[:,i]\n",
    "#                 samples_df['fft_feq_' + str(i)] = nlargest_idx_slice[:,i]\n",
    "\n",
    "            band1_slice = x_realfft[:,600:900]\n",
    "            band2_slice = x_realfft[:,1100:1450]\n",
    "            band3_slice = x_realfft[:,1450:1900]\n",
    "            band4_slice = x_realfft[:,1950:2800]\n",
    "            band5_slice = x_realfft[:,33000:42000]\n",
    "\n",
    "            samples_df['band1_alt' + suffix] = np.max(band1_slice,axis = 1)\n",
    "            samples_df['band1_freq' + suffix] = np.argmax(band1_slice,axis = 1)\n",
    "            samples_df['band2_alt' + suffix] = np.max(band2_slice,axis = 1)\n",
    "            samples_df['band2_freq' + suffix] = np.argmax(band2_slice,axis = 1)\n",
    "            samples_df['band3_alt' + suffix] = np.max(band3_slice,axis = 1)\n",
    "            samples_df['band3_freq' + suffix] = np.argmax(band3_slice,axis = 1)\n",
    "            samples_df['band4_alt' + suffix] = np.max(band4_slice,axis = 1)\n",
    "            samples_df['band4_freq' + suffix] = np.argmax(band4_slice,axis = 1)\n",
    "            samples_df['band5_alt' + suffix] = np.max(band5_slice,axis = 1)\n",
    "            samples_df['band5_freq' + suffix] = np.argmax(band5_slice,axis = 1)            \n",
    "\n",
    "\n",
    "            #abs\n",
    "#             samples_df['mean_abs'+suffix] = np.mean(current_slice_abs,axis=1)\n",
    "            samples_df['median_abs'+suffix] = np.median(current_slice_abs,axis=1)\n",
    "            samples_df['std_abs'+suffix] = np.std(current_slice_abs,axis=1)\n",
    "\n",
    "#             samples_df['kurtosis_abs'+suffix] = stats.kurtosis(current_slice_abs,axis=1)\n",
    "#             samples_df['m2_abs'+suffix] = stats.moment(current_slice_abs,axis=1,moment=2)\n",
    "#             samples_df['m3_abs'+suffix] = stats.moment(current_slice_abs,axis=1,moment=3)\n",
    "#             samples_df['skew_abs'+suffix] = stats.skew(current_slice_abs,axis=1)\n",
    "#             samples_df['variation_abs'+suffix] = stats.variation(current_slice_abs,axis=1)\n",
    "#             samples_df['sem_abs'+suffix] = stats.sem(current_slice_abs,axis=1)\n",
    "\n",
    "\n",
    "#             samples_df['iqr25_75_abs'+suffix] = stats.iqr(current_slice_abs,axis=1,rng=(25,75))\n",
    "            samples_df['iqr10_90_abs'+suffix] = stats.iqr(current_slice_abs,axis=1,rng=(10,90))\n",
    "#             samples_df['iqr5_95_abs'+suffix] = stats.iqr(current_slice_abs,axis=1,rng=(5,95))\n",
    "#             samples_df['iqr1_99_abs'+suffix] = stats.iqr(current_slice_abs,axis=1,rng=(1,99))            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# all_samples = random_samples.append(consecutive_samples)\n",
    "generateFullFeatures(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_samples.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples['band5_freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/consecutive_samples.pickle','wb') as f:\n",
    "    pickle.dump(consecutive_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/notebooks/storage/earthquake/random_samples.pickle','wb') as f:\n",
    "    pickle.dump(random_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_samples = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/consecutive_samples.pickle','rb') as f:\n",
    "    consecutive_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.plot(x='iqr1_99_abs',y='ttf',kind='scatter',logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.plot.hexbin(x='mean',y='ttf',gridsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(consecutive_samples['ttf'],consecutive_samples['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(consecutive_samples.ttf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='iqr1_99_abs',y='ttf',data=consecutive_samples,kind='hex',gridsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(consecutive_samples[['ttf','iqr1_99_abs','iqr5_95_abs','iqr10_90_abs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = fft(random_samples['sequence'].values[10][:5000])\n",
    "xvals = np.arange(0,5000)\n",
    "sns.jointplot(x = xvals,y = ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into plotly for interactive plots, and ggplot for grammar of graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_mean = 4.017 \n",
    "train_mean = 5.68\n",
    "mul_factor = public_mean/train_mean\n",
    "\n",
    "\n",
    "X = random_samples.drop(labels=['ttf','sequence'],axis=1)\n",
    "Y = random_samples['ttf']/mul_factor\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y,test_size=.1,random_state=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "n_estimators = 10000\n",
    "learning_rate = .001\n",
    "n_jobs = 8\n",
    "\n",
    "early_stopping_rounds = 20\n",
    "eval_set = [(x_val,y_val)]\n",
    "verbose = True\n",
    "\n",
    "model_xgb = XGBRegressor(\n",
    "    n_estimators = n_estimators, \n",
    "    learning_rate = learning_rate, \n",
    "    n_job = n_jobs\n",
    ")\n",
    "\n",
    "model_xgb.fit(x_train,y_train,\n",
    "              early_stopping_rounds= early_stopping_rounds,\n",
    "              eval_set = eval_set,\n",
    "              eval_metric = 'mae',\n",
    "              verbose = verbose\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = model_xgb.predict(x_val)\n",
    "print('mae : '+str(mean_absolute_error(predictions,y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERWRITE pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/model_xgb.pickle','wb') as f:\n",
    "    pickle.dump(model_xgb,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD pickled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/model_xgb.pickle','rb') as f:\n",
    "    model_xgb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create submission\n",
    "#### start with creating a dataframe identical in format to consecutive/random samples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.chdir('/notebooks/storage/earthquake/test')\n",
    "# os.remove(\"submission.csv\")\n",
    "test_samples = pd.DataFrame(columns=['sequence','ttf'])\n",
    "\n",
    "files = [x for x in os.listdir() if x[-4:] == '.csv']\n",
    "\n",
    "for i in range(len(files)):\n",
    "    \n",
    "    test_file = files[i]\n",
    "    \n",
    "    temp_df = pd.read_csv('./'+test_file,engine='python')\n",
    "        \n",
    "    test_samples.loc[i,'sequence'] = np.array(temp_df['acoustic_data'].values[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVERWRITE pickle test sample df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('/notebooks/storage/earthquake/test_samples.pickle','wb') as f:\n",
    "#     pickle.dump(test_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD pickled test sample df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/test_samples.pickle','rb') as f:\n",
    "    test_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "generateFullFeatures(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle engineered sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('/notebooks/storage/earthquake/test_samples_eng.pickle','wb') as f:\n",
    "#     pickle.dump(test_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pickled engineered sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('/notebooks/storage/earthquake/test_samples_eng.pickle','rb') as f:\n",
    "    test_samples_eng = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make predictions/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_samples_eng.drop(columns=['sequence','ttf'])\n",
    "\n",
    "y_pred = model_xgb.predict(test_x)\n",
    "\n",
    "files = [x for x in os.listdir('/notebooks/storage/earthquake/test') if x[-4:] == '.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns=['seg_id','time_to_failure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['seg_id'] = pd.Series([i[:-4] for i in files])\n",
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['time_to_failure'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('/notebooks/storage/earthquake/submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
