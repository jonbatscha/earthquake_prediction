{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL EARTHQUAKE PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## packages and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "import pickle\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy import fftpack\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # may remove later\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set project parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "PROJECT_DIR = Path('/notebooks/storage/earthquake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change `pwd` to `PROJECT_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download/unzip/load data from kaggle\n",
    "- run once -> comment out\n",
    "- ADD: NOTE ON WHAT HAPPENS IF IT IS RUN TWICE, REPLACEMENT OR COLLISION ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !kaggle competitions download LANL-Earthquake-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unzip kaggle files\n",
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_zip='train.csv.zip'\n",
    "# test_zip='test.zip'\n",
    "# earthquake_dir='.'\n",
    "# test_dir='./test'\n",
    "\n",
    "# zip_ref=zipfile.ZipFile(train_zip,'r')\n",
    "# zip_ref.extractall(earthquake_dir)\n",
    "# zip_ref.close()\n",
    "\n",
    "# zip_ref=zipfile.ZipFile(test_zip,'r')\n",
    "# zip_ref.extractall(test_dir)\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read train csv -> convert to dataframe -> pickle\n",
    "NOTE: run once -> pickle `train` -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# train = pd.read_csv('./train.csv', dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `train` dataframe\n",
    "- NOTE: run once then comment out\n",
    "- CAUTION !!! code below overwrites `train`!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open('/notebooks/storage/earthquake/train.pickle','wb') as f:\n",
    "#     pickle.dump(train,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `train` dataframe from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load `train` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(PROJECT_DIR/'train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate consecutive samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set `consecutive_sample` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "CONS_FILENAME = 'consecutive_samples'\n",
    "\n",
    "#leave as-is\n",
    "cons_filename = PROJECT_DIR/str(CONS_FILENAME+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateConsecutive(train):\n",
    "    '''\n",
    "    input: train dataframe (dim: #entries x 2 features: acoustic_data/time_to_failure)\n",
    "    returns: dataframe with length/sample_length consecutive samples, each with:\n",
    "        sequence = np.array(150k,) and time_to_failure = float\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(length//sample_length):\n",
    "        \n",
    "        start = i*sample_length\n",
    "        end = (i+1)*sample_length\n",
    "        \n",
    "        samples += [[train['acoustic_data'].values[start:end],train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'time_to_failure'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "consecutive_samples = generateConsecutive(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `consecutive_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open(cons_filename,'wb') as f:\n",
    "#     pickle.dump(consecutive_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `consecutive_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del consecutive_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `consecutive_samples` from PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(cons_filename,'rb') as f:\n",
    "    consecutive_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate N random samples\n",
    "with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "RAND_FILENAME = 'random_samples'\n",
    "\n",
    "#leave unchanged\n",
    "rand_filename = PROJECT_DIR/str(RAND_FILENAME+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandom(train,n):\n",
    "    '''\n",
    "    input: train dataframe (dim: #entries x 2 features: acoustic_data/time_to_failure)\n",
    "    returns: dataframe with n random samples, each with:\n",
    "        sequence = np.array(150k,) and time_to_failure = float\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        start = random.randint(0,max_start)\n",
    "        end = start + sample_length\n",
    "        \n",
    "        samples += [[np.array(train['acoustic_data'].values[start:end]),train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'time_to_failure'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random_samples = generateRandom(train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `random_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(rand_filename,'wb') as f:\n",
    "    pickle.dump(random_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `random_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del random_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `consecutive_samples` from `PROJECT_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(rand_filename,'rb') as f:\n",
    "    random_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate leaked samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "LEAKED_FILENAME = 'leaked_samples'\n",
    "N_LEAKED_SAMPLES = 10\n",
    "\n",
    "#leave unchanged\n",
    "leaked_filename = PROJECT_DIR/str(LEAKED_FILENAME+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthquake times\n",
    "eq0 = (0,5656574)\n",
    "eq3 = (104677355,138772454)\n",
    "eq5 = (187641819,218652631)\n",
    "eq6 = (218652629,245829586)\n",
    "eq8 = (307838916,338276288)\n",
    "eq12 = (461811622,495800226)\n",
    "eq13 = (495800224,528777116)\n",
    "eq16 = (621985672,629145480)\n",
    "\n",
    "nogo = [eq0,eq16]\n",
    "nogo1 = nogo + [eq3,eq5,eq6]\n",
    "nogo2 = nogo + [eq3,eq8,eq12,eq13]\n",
    "nogo3 = nogo + [eq6,eq8,eq13]\n",
    "nogo4 = nogo + [eq6,eq8,eq12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(start,end,nogo_list):\n",
    "    \n",
    "    valid = True\n",
    "    \n",
    "    for a,b in nogo_list:\n",
    "        \n",
    "        if start>a and start<b:\n",
    "            valid = False\n",
    "        if end>a and end<b:\n",
    "            valid = False\n",
    "        \n",
    "    return valid\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLeaked(train,n,nogo_list):\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    length = len(train)\n",
    "    sample_length = 150000\n",
    "    max_index = length-1\n",
    "    max_start = max_index - sample_length\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        start = random.randint(0,max_start)\n",
    "        end = start + sample_length\n",
    "        \n",
    "        while not is_valid(start,end,nogo_list):\n",
    "            start = random.randint(0,max_start)\n",
    "            end = start + sample_length\n",
    "        \n",
    "        samples += [[np.array(train['acoustic_data'].values[start:end]),train['time_to_failure'].values[end]]]\n",
    "                \n",
    "    df = pd.DataFrame(samples)\n",
    "        \n",
    "    df = df.rename(columns = {0:'sequence',1:'time_to_failure'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "leaked_samples_list = []\n",
    "\n",
    "nogos = [nogo1,nogo2,nogo3,nogo4]\n",
    "\n",
    "for nogo in nogos:\n",
    "\n",
    "    current_sample = generateLeaked(train,N_LEAKED_SAMPLES,nogo)\n",
    "    \n",
    "    leaked_samples_list += [current_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_samples_list[3].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_samples = pd.concat(leaked_samples_list, axis=0)\n",
    "leaked_samples = leaked_samples.reset_index()\n",
    "leaked_samples = leaked_samples.drop(labels=['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_samples.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_samples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `leaked_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open(leaked_filename,'wb') as f:\n",
    "#     pickle.dump(leaked_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `leaked_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del leaked_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `leaked_samples` from `PROJECT_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(leaked_filename,'rb') as f:\n",
    "    leaked_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaked_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full sequence features\n",
    "add features to a given sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullFeatures(samples_df):\n",
    "    \n",
    "    #defining x as matrix of sequence data\n",
    "    x = []\n",
    "    for index,row in samples_df.iterrows():\n",
    "        x += [samples_df.loc[index,'sequence']]    \n",
    "    x = np.stack(x)\n",
    "    x_abs = np.absolute(x)\n",
    "    \n",
    "    length=150000\n",
    "    \n",
    "    #non-abs\n",
    "    samples_df['mean'] = np.mean(x,axis=1)\n",
    "    samples_df['median'] = np.median(x,axis=1)\n",
    "    samples_df['std'] = np.std(x,axis=1)\n",
    "    \n",
    "    samples_df['kurtosis'] = stats.kurtosis(x,axis=1)   \n",
    "    samples_df['m2'] = stats.moment(x,axis=1,moment=2)\n",
    "    samples_df['m3'] = stats.moment(x,axis=1,moment=3)\n",
    "    samples_df['skew'] = stats.skew(x,axis=1)\n",
    "    samples_df['variation'] = stats.variation(x,axis=1)\n",
    "    samples_df['sem'] = stats.sem(x,axis=1)\n",
    "\n",
    "    samples_df['iqr25_75'] = stats.iqr(x,axis=1,rng=(25,75))\n",
    "    samples_df['iqr10_90'] = stats.iqr(x,axis=1,rng=(10,90))\n",
    "    samples_df['iqr5_95'] = stats.iqr(x,axis=1,rng=(5,95))\n",
    "    samples_df['iqr1_99'] = stats.iqr(x,axis=1,rng=(1,99))\n",
    "    \n",
    "    \n",
    "    #abs\n",
    "    samples_df['mean_abs'] = np.mean(x_abs,axis=1)\n",
    "    samples_df['median_abs'] = np.median(x_abs,axis=1)\n",
    "    samples_df['std_abs'] = np.std(x_abs,axis=1)\n",
    "    \n",
    "    samples_df['kurtosis_abs'] = stats.kurtosis(x_abs,axis=1)   \n",
    "    samples_df['m2_abs'] = stats.moment(x_abs,axis=1,moment=2)\n",
    "    samples_df['m3_abs'] = stats.moment(x_abs,axis=1,moment=3)\n",
    "    samples_df['skew_abs'] = stats.skew(x_abs,axis=1)\n",
    "    samples_df['variation_abs'] = stats.variation(x_abs,axis=1)\n",
    "    samples_df['sem_abs'] = stats.sem(x_abs,axis=1)\n",
    "\n",
    "    \n",
    "    samples_df['iqr25_75_abs'] = stats.iqr(x_abs,axis=1,rng=(25,75))\n",
    "    samples_df['iqr10_90_abs'] = stats.iqr(x_abs,axis=1,rng=(10,90))\n",
    "    samples_df['iqr5_95_abs'] = stats.iqr(x_abs,axis=1,rng=(5,95))\n",
    "    samples_df['iqr1_99_abs'] = stats.iqr(x_abs,axis=1,rng=(1,99))\n",
    "    \n",
    "    \n",
    "    #slices\n",
    "    slices_list = [2,4]\n",
    "    \n",
    "    for slices in slices_list:\n",
    "\n",
    "        for i in range(slices):\n",
    "\n",
    "            suffix = '_'+str(slices)+'_'+str(i+1)\n",
    "\n",
    "            #create same as above, but for first half and second half\n",
    "            \n",
    "            x_slice = x[:,i*(length//slices):(i+1)*(length//slices)]\n",
    "            x_abs_slice = np.absolute(x_slice)\n",
    "\n",
    "            #non-abs\n",
    "            samples_df['mean'+suffix] = np.mean(x_slice,axis=1)\n",
    "            samples_df['median'+suffix] = np.median(x_slice,axis=1)\n",
    "            samples_df['std'+suffix] = np.std(x_slice,axis=1)\n",
    "\n",
    "            samples_df['kurtosis'+suffix] = stats.kurtosis(x_slice,axis=1)\n",
    "            samples_df['m2'+suffix] = stats.moment(x_slice,axis=1,moment=2)\n",
    "            samples_df['m3'+suffix] = stats.moment(x_slice,axis=1,moment=3)\n",
    "            samples_df['skew'+suffix] = stats.skew(x_slice,axis=1)\n",
    "            samples_df['variation'+suffix] = stats.variation(x_slice,axis=1)\n",
    "            samples_df['sem'+suffix] = stats.sem(x_slice,axis=1)\n",
    "\n",
    "            samples_df['iqr25_75'+suffix] = stats.iqr(x_slice,axis=1,rng=(25,75))\n",
    "            samples_df['iqr10_90'+suffix] = stats.iqr(x_slice,axis=1,rng=(10,90))\n",
    "            samples_df['iqr5_95'+suffix] = stats.iqr(x_slice,axis=1,rng=(5,95))\n",
    "            samples_df['iqr1_99'+suffix] = stats.iqr(x_slice,axis=1,rng=(1,99))\n",
    "\n",
    "\n",
    "            #abs\n",
    "            samples_df['mean_abs'+suffix] = np.mean(x_abs_slice,axis=1)\n",
    "            samples_df['median_abs'+suffix] = np.median(x_abs_slice,axis=1)\n",
    "            samples_df['std_abs'+suffix] = np.std(x_abs_slice,axis=1)\n",
    "\n",
    "            samples_df['kurtosis_abs'+suffix] = stats.kurtosis(x_abs_slice,axis=1)\n",
    "            samples_df['m2_abs'+suffix] = stats.moment(x_abs_slice,axis=1,moment=2)\n",
    "            samples_df['m3_abs'+suffix] = stats.moment(x_abs_slice,axis=1,moment=3)\n",
    "            samples_df['skew_abs'+suffix] = stats.skew(x_abs_slice,axis=1)\n",
    "            samples_df['variation_abs'+suffix] = stats.variation(x_abs_slice,axis=1)\n",
    "            samples_df['sem_abs'+suffix] = stats.sem(x_abs_slice,axis=1)\n",
    "\n",
    "\n",
    "            samples_df['iqr25_75_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(25,75))\n",
    "            samples_df['iqr10_90_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(10,90))\n",
    "            samples_df['iqr5_95_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(5,95))\n",
    "            samples_df['iqr1_99_abs'+suffix] = stats.iqr(x_abs_slice,axis=1,rng=(1,99))            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFTSingle(index,sequence,samples_df):\n",
    "    \n",
    "    x_realfft = np.real(fftpack.fft(sequence))\n",
    "    \n",
    "    band1 = x_realfft[600:900]\n",
    "    band2 = x_realfft[1100:1450]\n",
    "    band3 = x_realfft[1450:1900]\n",
    "    band4 = x_realfft[1950:2800]\n",
    "    band5 = x_realfft[33000:42000]\n",
    "    \n",
    "    samples_df.loc[index,'band1_alt'] = np.max(band1)\n",
    "    samples_df.loc[index,'band1_freq'] = np.argmax(band1)\n",
    "    samples_df.loc[index,'band2_alt'] = np.max(band2)\n",
    "    samples_df.loc[index,'band2_freq'] = np.argmax(band2)\n",
    "    samples_df.loc[index,'band3_alt'] = np.max(band3)\n",
    "    samples_df.loc[index,'band3_freq'] = np.argmax(band3)\n",
    "    samples_df.loc[index,'band4_alt'] = np.max(band4)\n",
    "    samples_df.loc[index,'band4_freq'] = np.argmax(band4)\n",
    "    samples_df.loc[index,'band5_alt'] = np.max(band5)\n",
    "    samples_df.loc[index,'band5_freq'] = np.argmax(band5)\n",
    "    \n",
    "def generateFFTFeatures(samples_df):\n",
    "    \n",
    "    for index, row in samples_df.iterrows():\n",
    "        \n",
    "        FFTSingle(index,row['sequence'],samples_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate features for a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit\n",
    "eng_samples = leaked_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit\n",
    "eng_samples_name = 'leaked_samples'\n",
    "\n",
    "#leave as-is\n",
    "eng_samples_filename = PROJECT_DIR/str(eng_samples_name+'_eng.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generateFullFeatures(eng_samples)\n",
    "generateFFTFeatures(eng_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### pickle `eng_samples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: run once -> comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open(eng_samples_filename,'wb') as f:\n",
    "#     pickle.dump(eng_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `eng_samples` from ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del eng_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `eng_samples` from PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(eng_samples_filename,'rb') as f:\n",
    "    eng_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_samples.plot(x='iqr1_99_abs',y='time_to_failure',kind='scatter',logx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_samples.plot.hexbin(x='mean',y='time_to_failure',gridsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(eng_samples['time_to_failure'],eng_samples['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(eng_samples.time_to_failure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='iqr1_99_abs',y='time_to_failure',data=eng_samples,kind='hex',gridsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(eng_samples[['time_to_failure','iqr1_99_abs','iqr5_95_abs','iqr10_90_abs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into ggplot for grammar of graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unwanted columns from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = eng_samples.drop(labels=['time_to_failure','sequence'],axis=1)\n",
    "Y = eng_samples['time_to_failure']\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y,test_size=.1,random_state=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10000\n",
    "learning_rate = .005\n",
    "n_jobs = 8\n",
    "\n",
    "early_stopping_rounds = 10\n",
    "eval_set = [(x_val,y_val)]\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators = n_estimators, \n",
    "    learning_rate = learning_rate, \n",
    "    n_job = n_jobs\n",
    ")\n",
    "\n",
    "model.fit(x_train,y_train,\n",
    "              early_stopping_rounds= early_stopping_rounds,\n",
    "              eval_set = eval_set,\n",
    "              eval_metric = 'mae',\n",
    "              verbose = verbose\n",
    "             )\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "predictions = model.predict(x_val)\n",
    "print('mae : '+str(mean_absolute_error(predictions,y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "MODEL_NAME = 'model_4096_all'\n",
    "\n",
    "#leave as-is\n",
    "model_filename = PROJECT_DIR/str(MODEL_NAME+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(model_filename,'wb') as f:\n",
    "    pickle.dump(model,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `model` from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pickled `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(model_filename,'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create submission\n",
    "#### start by creating a dataframe identical in format to consecutive/random samples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit\n",
    "TEST_SAMPLES_NAME = 'test_samples'\n",
    "\n",
    "# leave as-is\n",
    "test_samples_filename = PROJECT_DIR/str(TEST_SAMPLES_NAME+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "os.chdir(PROJECT_DIR/'test')\n",
    "    \n",
    "test_samples = pd.DataFrame(columns=['sequence','time_to_failure']) #change to 'time_to_failure'\n",
    "\n",
    "for i in range(len(os.listdir())):\n",
    "    \n",
    "    test_file = os.listdir()[i]\n",
    "    \n",
    "    temp_df = pd.read_csv('./'+test_file,engine='python')\n",
    "        \n",
    "    test_samples.loc[i,'sequence'] = np.array(temp_df['acoustic_data'].values[:])\n",
    "\n",
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `test_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with open(test_samples_filename,'wb') as f:\n",
    "#     pickle.dump(test_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `test_samples` from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load `test_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(test_samples_filename,'rb') as f:\n",
    "    test_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generateFullFeatures(test_samples)\n",
    "generateFFTFeatures(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle `test_samples_eng`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(PROJECT_DIR/str(TEST_SAMPLES_NAME+'_eng.pickle'),'wb') as f:\n",
    "    pickle.dump(test_samples,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete `test_samples_eng` from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load `test_samples_eng`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(PROJECT_DIR/str(TEST_SAMPLES_NAME+'_eng.pickle'),'rb') as f:\n",
    "    test_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions/submit\n",
    "This code will create a `submission.csv` file, that can be submitted to kaggle using the command line api, or website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_samples.drop(columns=['sequence','time_to_failure'])\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns=['seg_id','time_to_failure'])\n",
    "\n",
    "submission_df['seg_id'] = pd.Series([i[:-4] for i in os.listdir(PROJECT_DIR/'test') if i[-4:]=='.csv'])\n",
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['time_to_failure'] = y_pred\n",
    "\n",
    "submission_df.to_csv(PROJECT_DIR/'submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
